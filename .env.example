############################################################
# App / Server
############################################################
# Port the API listens on (gunicorn / uvicorn worker)
PORT=5000

# Logging level: DEBUG | INFO | WARNING | ERROR | CRITICAL
LOG_LEVEL=INFO

# Where the API serves static files from (only used if API serves UI directly)
# In Docker + Nginx, UI is served by Nginx at /usr/share/nginx/html
# For local dev (no nginx), you can point this to ./ui/dist after `npm run build`
STATIC_ROOT=./ui/dist


############################################################
# CORS
############################################################
# For development: allow everything. For production, restrict to your domains.
CORS_ALLOW_ORIGINS=*
CORS_ALLOW_METHODS=*
CORS_ALLOW_HEADERS=*
# CORS_ALLOW_CREDENTIALS=true


############################################################
# Storage (local file paths mounted in containers)
############################################################
DATA_DIR=./data
DOCMAP_DIR=./data/docmaps
JOBS_DIR=./data/jobs
CHROMA_DIR=./data/chroma

# Disable anonymous telemetry from Chroma (optional)
ANONYMIZED_TELEMETRY=False


############################################################
# Redis / RQ
############################################################
# Redis URL:
# - Local dev (host machine): redis://localhost:6379/0
# - In Docker Compose (services talk over the network): redis://redis:6379/0
REDIS_URL=redis://localhost:6379/0

# Default RQ queue name
RQ_QUEUE=jobs

# Worker class:
# - Linux/CI/Prod: rq.worker.Worker  (forked, default)
# - macOS local dev: rq.worker.SimpleWorker  (no fork; avoids Obj-C fork crash)
# You can override per-environment.
RQ_WORKER_CLASS=rq.worker.Worker

# Optional worker metadata and tuning
RQ_WORKER_NAME=local-worker
RQ_RESULT_TTL=3600
RQ_FAILURE_TTL=3600
RQ_LOG_LEVEL=INFO

# macOS-only: if you force forked workers and see Obj-C fork crash, uncomment:
# OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES


############################################################
# Security / Auth
############################################################
# Simple API key used by the backend (adjust according to your auth strategy)
API_KEY=change-me-dev-key
# Or multiple keys (comma-separated):
# API_KEYS=change-me-dev-key,another-key

# Default tenant used by examples/tests
TENANT=public
DEFAULT_TENANT=public

# --- Optional JWT/JWKS ---
# JWT_SECRET=change-me
# JWKS_URL=https://your-idp/.well-known/jwks.json
# JWT_AUDIENCE=your-audience
# JWT_ISSUER=https://your-idp/


############################################################
# watsonx.ai (REQUIRED â€” replace all placeholders)
############################################################
# CrewAI / LiteLLM read BOTH `WATSONX_APIKEY` and `WATSONX_API_KEY`.
# Keep them in sync to avoid provider fallback errors.
WATSONX_APIKEY=<your-watsonx-api-key>
WATSONX_API_KEY=<your-watsonx-api-key>

# Base URL (derived from region). Examples: us-south, eu-de, etc.
WATSONX_REGION=us-south
WATSONX_URL=https://us-south.ml.cloud.ibm.com

# Project ID from IBM Cloud watsonx.ai
WATSONX_PROJECT_ID=<your-watsonx-project-id>

# Chat/completion model
# Example: meta-llama/llama-3-3-70b-instruct (ensure your account has access)
WATSONX_CHAT_MODEL=meta-llama/llama-3-3-70b-instruct

# Embedding model (choose one)
# ibm/text-embedding-001 is a good default
WATSONX_EMBED_MODEL=ibm/text-embedding-001
# Alternative (shorter context): ibm/slate-125m-english-rtrvr
# WATSONX_EMBED_MODEL=ibm/slate-125m-english-rtrvr


############################################################
# Crew / generation defaults
############################################################
# Explicitly choose the LLM provider for CrewAI agents: watsonx | openai | ollama
# We default to watsonx to match the rest of this project.
CREW_PROVIDER=watsonx

# Temperature for LLM generation (0.0 - 1.0)
CREW_TEMPERATURE=0.2

# --- Optional: OpenAI provider ---
# OPENAI_API_KEY=sk-...
# OPENAI_MODEL=gpt-4o-mini

# --- Optional: Ollama provider (local models) ---
# OLLAMA_HOST=http://localhost:11434
# OLLAMA_MODEL=llama3.1


############################################################
# Frontend (Vite)
############################################################
# Base URL the UI uses to reach the API.
# If UI and API are served behind the same Nginx host, /api is correct.
VITE_API_BASE_URL=/api
# If running UI separately in dev mode, you might use:
# VITE_API_BASE_URL=http://localhost:5000/api
