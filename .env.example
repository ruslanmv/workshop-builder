############################################################
# App / Server
############################################################
# Port the API listens on (gunicorn / uvicorn worker)
PORT=5000

# Logging level: DEBUG | INFO | WARNING | ERROR | CRITICAL
LOG_LEVEL=INFO

# Where the API serves static files from (only used if API serves UI directly)
# In Docker + Nginx, UI is served by Nginx at /usr/share/nginx/html
# For local dev (no nginx), you can point this to ./ui/dist after `npm run build`
STATIC_ROOT=./ui/dist


############################################################
# CORS
############################################################
# For development: allow everything. For production, restrict to your domains.
CORS_ALLOW_ORIGINS=*
CORS_ALLOW_METHODS=*
CORS_ALLOW_HEADERS=*


############################################################
# Storage (local file paths mounted in containers)
############################################################
DATA_DIR=./data
DOCMAP_DIR=./data/docmaps
JOBS_DIR=./data/jobs
CHROMA_DIR=./data/chroma

# Disable anonymous telemetry from Chroma (optional)
ANONYMIZED_TELEMETRY=False


############################################################
# Redis / RQ
############################################################
# Redis URL:
# - Local dev (host machine): redis://localhost:6379/0
# - In Docker Compose (services talk over the network): redis://redis:6379/0
REDIS_URL=redis://localhost:6379/0

# Default RQ queue name
RQ_QUEUE=jobs

# Worker class:
# - Linux/CI/Prod: rq.worker.Worker  (forked, default)
# - macOS local dev: rq.worker.SimpleWorker  (no fork; avoids Obj-C fork crash)
# You can override per-environment.
RQ_WORKER_CLASS=rq.worker.Worker

# Optional worker metadata
RQ_WORKER_NAME=local-worker

# Optional TTL tuning (seconds)
RQ_RESULT_TTL=3600
RQ_FAILURE_TTL=3600

# macOS-only: if you force forked workers and see Obj-C fork crash, uncomment:
# OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES


############################################################
# Security / Auth
############################################################
# Simple API key used by the backend (adjust according to your auth strategy)
API_KEY=change-me-dev-key

# Default tenant used by examples/tests
TENANT=public


############################################################
# watsonx.ai (REQUIRED â€” replace all placeholders)
############################################################
# Get these from IBM Cloud > watsonx.ai service instance
WATSONX_API_KEY=<your-watsonx-api-key>
WATSONX_PROJECT_ID=<your-watsonx-project-id>
# Region example: us-south, eu-de, etc.
WATSONX_REGION=us-south

# Chat/completion model
# Example: meta-llama/llama-3-3-70b-instruct (ensure your account has access)
WATSONX_CHAT_MODEL=meta-llama/llama-3-3-70b-instruct

# Embedding model (choose one)
# ibm/text-embedding-001 is a good default
WATSONX_EMBED_MODEL=ibm/text-embedding-001
# Alternative (shorter context): ibm/slate-125m-english-rtrvr
# WATSONX_EMBED_MODEL=ibm/slate-125m-english-rtrvr


############################################################
# Crew / generation defaults
############################################################
# Temperature for LLM generation (0.0 - 1.0)
CREW_TEMPERATURE=0.2


############################################################
# Frontend (Vite)
############################################################
# Base URL the UI uses to reach the API.
# If UI and API are served behind the same Nginx host, /api is correct.
VITE_API_BASE_URL=/api
# If running UI separately in dev mode, you might use:
# VITE_API_BASE_URL=http://localhost:5000/api
